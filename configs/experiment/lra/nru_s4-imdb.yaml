# @package _global_
defaults:
  - /pipeline: imdb
  - /model: nru
  - override /scheduler: cosine_warmup

model:
  dropout: 0.1
  tie_dropout: true
  n_layers: 6
  d_model: 256
  prenorm: true
  norm: batch
  layer:
    gate: null # 4 for GSS
    gate_act: id # 4 for GSS
    bottleneck: null # 4 for GSS
    activation: gelu
    mult_act: null
    final_act: glu # Final activation after FF layer; new name for 'postact'
    postact: null  # Deprecated for 'final_act'
    initializer: null
    weight_norm: false
    d_memory: 256
    layer: s4nru
    layer_args:
      bidirectional: true
      d_state: 4
      channels: 1
      gate: null # 4 for GSS
      gate_act: id # 4 for GSS
      bottleneck: null # 4 for GSS
      activation: gelu
      mult_act: null
      final_act: glu # Final activation after FF layer; new name for 'postact'
      postact: null  # Deprecated for 'final_act'
      initializer: null
      weight_norm: false
      # dropout: ${model.dropout} # Same as null
      tie_dropout: ${oc.select:model.tie_dropout,null}
      # Layer arguments
      layer: fftconv
      # Kernel arguments
      mode: nplr
      init: legs  # Initialization option; new name for 'measure'
      measure: null  # Deprecated for 'init'
      rank: 1
      dt_min: 0.001
      dt_max: 0.1
      dt_transform: softplus  # Use 'exp' for any backwards-compatible experiments
      lr:
        dt: null
        A: 0.001
        B: 0.001
      wd: 0.0 # Can be dictionary like lr
      n_ssm: 1
      drop_kernel: 0.0
      deterministic: false # Special C init
      l_max: ${oc.select:dataset.__l_max,null} # Grab dataset length if exists, otherwise set to 1 and kernel will automatically resize
      verbose: true

dataset:
  l_max: 4096
  level: char

loader:
  batch_size: 16

optimizer:
  lr: 0.01
  weight_decay: 0.05

scheduler:
  num_training_steps: 50000
  num_warmup_steps: 5000

trainer:
  max_epochs: 32

train:
  seed: 3333

decoder:
  mode: pool